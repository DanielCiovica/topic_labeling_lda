{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Project 1 Probabilistic Programming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Daniel\n[nltk_data]     Ciovica\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to C:\\Users\\Daniel\n[nltk_data]     Ciovica\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "import re\n",
    "import pymc as pm\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "source": [
    "To implement the first and the second task I have created a class called LDA. There, I have initialized the required dimensions and created alpha, beta, phi, theta, z and w as described in task 1, as well as the following functions that are to be described: \n",
    "\n",
    "def __init__ :\n",
    "\n",
    "- K is the number of topics\n",
    "- V is the size of the vocabulary\n",
    "- M is the number of documents\n",
    "- N contains the number of words for each document\n",
    "- Phi is a container composed of a completed dirichlet that has a prior distribution as a dirichlet of length K\n",
    "- Theta is a container composed of a completed dirichlet that has a prior distribution as a dirichlet of length M\n",
    "- Z is a contianer composed of a categorical distribution with probabilities theta, having K values of size N[i], where i is the number of words of the i th document\n",
    "- w is a contianer composed of a categorical distribution of a lambda function that is determined using phi[z] and z[i][j], where i, j are the i^th document and j^th number of i^th document . To the categorical distribution the data and the observed=True parameter are given. \n",
    "\n",
    "def compile :\n",
    "\n",
    "- create the model using all the above described parameters\n",
    "- create the mcmc model\n",
    "- sample the model according to the set iterations and burn\n",
    "\n",
    "def trace :\n",
    "\n",
    "- trace the phi, theta and z samples and display them\n",
    "\n",
    "def topic_words:\n",
    "\n",
    "- using the phi values, I have displayed the 5 most representative words for each topic\n",
    "\n",
    "def hellinger_distance:\n",
    "\n",
    "- to calculate the similarity between two documents of the same topic, I have used the Hellinger Distance that calculates the difference between the square root of every element of the same index of the first and second document, everything squared. \n",
    "\n",
    "def documents_similarity:\n",
    "\n",
    "- iterating through the traced theta samples, search for the corresponding documnets and call the function that calculates the dellinger distance\n",
    "\n",
    "def assign_topics_new_documnets:\n",
    "\n",
    "- iterating through all the new data, add the traced phi values corresponding to the new provided words to get the probability of every document belonging to each topic.\n",
    "- predict the topic by selecting the one with the maximum value \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    def __init__(self, data, K, V, N, M, iterations, burn, second_df=None):\n",
    "        self.data = data\n",
    "        self.K = K\n",
    "        self.V = V\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "        self.iterations = iterations\n",
    "        self.burn = burn\n",
    "\n",
    "        self.alpha = np.ones(self.K)\n",
    "        self.beta = np.ones(self.V)\n",
    "        \n",
    "        self.theta_prior = pm.Container([pm.Dirichlet(\"theta_prior_%s\" % i, theta = self.alpha) for i in range(self.M)])\n",
    "        self.theta = pm.Container([pm.CompletedDirichlet(\"theta_%s\" % i, self.theta_prior[i]) for i in range(self.M)])\n",
    "\n",
    "        self.phi_prior = pm.Container([pm.Dirichlet(\"phi_prior_%s\" % i, theta=self.beta) for i in range(self.K)])\n",
    "        self.phi = pm.Container([pm.CompletedDirichlet(\"phi_%s\" %i, self.phi_prior[i]) for i in range(self.K)])\n",
    "\n",
    "        self.z = pm.Container([pm.Categorical(\"z_%s\" %i, p = self.theta[i], \n",
    "                                size = self.N[i],\n",
    "                                value = np.random.randint(self.K, size=self.N[i]))\n",
    "                                for i in range(self.M)\n",
    "                                ])\n",
    "        self.w = pm.Container([pm.Categorical(\"w_%s, %s\" % (i, j),\n",
    "                                p = pm.Lambda(\"phi_z_%s_%s\" % (i, j),\n",
    "                                lambda z=self.z[i][j],\n",
    "                                phi = self.phi:phi[z]),\n",
    "                                value = self.data[i][j],\n",
    "                                observed = True)\n",
    "                                for i in range(self.M) for j in range(self.N[i])\n",
    "                                ])\n",
    "\n",
    "    def compile(self):\n",
    "        self.model = pm.Model([self.theta, self.phi, self.z, self.w, self.theta_prior, self.phi_prior])\n",
    "        self.mcmc = pm.MCMC(self.model)\n",
    "        self.mcmc.sample(self.iterations, self.burn)\n",
    "\n",
    "    def trace(self):\n",
    "        self.phi_samples = [self.mcmc.trace(\"phi_%s\" %k)[:].mean(axis=0) for k in range(self.K)]\n",
    "        self.theta_samples = [self.mcmc.trace(\"theta_%s\" %m)[:].mean(axis=0) for m in range(self.M)]\n",
    "        self.z_samples = [np.round(self.mcmc.trace(\"z_%s\" %k)[:].mean(axis=0)) for k in range(self.M)]\n",
    "        print(\"\\n\\n Phi\")\n",
    "        for phi in self.phi_samples:\n",
    "            print(\"\\n\", phi)\n",
    "        print(\"\\n Theta\")\n",
    "        for theta in self.theta_samples:\n",
    "            print(\"\\n\", theta)\n",
    "        print(\"\\n Z\")\n",
    "        for z in self.z_samples:\n",
    "            print(\"\\n\", z)\n",
    "    \n",
    "    def topic_words(self):\n",
    "        words_per_topic = []\n",
    "        for index, topic in enumerate(self.phi.value):\n",
    "            words_per_topic.append(np.argsort(topic[0])[-5:])\n",
    "        return(words_per_topic)\n",
    "\n",
    "    @staticmethod\n",
    "    def hellinger_distance(doc_1, doc_2):\n",
    "        result = 0\n",
    "        for index in range(len(doc_1[0])):\n",
    "            result += pow((math.sqrt(doc_1[0][index]) - (math.sqrt(doc_2[0][index]))), 2)\n",
    "        return result\n",
    "\n",
    "    def documents_similarity(self, threshold):\n",
    "        similarities = []\n",
    "        for index_1, doc_1 in enumerate(self.theta_samples):\n",
    "            for index_2, doc_2 in enumerate(self.theta_samples):\n",
    "                if index_1 % 2 == 0 and index_2 %2 ==1  and index_1 + 1== index_2:\n",
    "                    similarities.append([index_1, index_2, 1 - self.hellinger_distance(doc_1, doc_2)])\n",
    "\n",
    "        similarities = [similarity for similarity in similarities if similarity[2] > threshold]\n",
    "\n",
    "        \n",
    "        for index, similarity in enumerate(similarities):\n",
    "            print(\"Topic {}: \".format(index))\n",
    "            print(\"{} \\n\".format(similarity))\n",
    "\n",
    "\n",
    "    def assign_topics_new_document(self, second_df):\n",
    "        topics = []\n",
    "        for i in range(len(second_df)):\n",
    "            result = []\n",
    "            for k in range(self.K):\n",
    "                topic_sum = 0\n",
    "                for word in second_df[i]:\n",
    "                    topic_sum += self.phi_samples[k][0][word]\n",
    "                result.append(topic_sum)\n",
    "            topics.append(np.argsort(result)[-1])\n",
    "        \n",
    "        print(topics)"
   ]
  },
  {
   "source": [
    "The following function processes the original data set by:\n",
    "\n",
    "- eliminating whitespace\n",
    "- eliminating the words containig numbers\n",
    "- get only the words that are not in the stop_words list\n",
    "- get rid of any punctuation\n",
    "- lemmatizing each word, by verb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeText(text):\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_text =  [w.lower() for w in w_tokenizer.tokenize(text) if bool(re.search(r'\\d', w)) == False]\n",
    "    new_text = [re.sub(r'[^\\w\\s]','',x) for x in new_text if x not in stop_words ]\n",
    "    aux_text =[lemmatizer.lemmatize(w, pos='v') for w in new_text]\n",
    "    return aux_text"
   ]
  },
  {
   "source": [
    "Because the original dataset is way too big, the following function selects only the first two documents for each topic, storing them in the first_df dataframe. The second_df will be used for assigning topics to new, unseen documents. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getData(df):\n",
    "\n",
    "    first_df = pd.DataFrame()\n",
    "    aux_df = pd.DataFrame()\n",
    "    second_df = pd.DataFrame()\n",
    "\n",
    "    for i in df['category'].unique():\n",
    "        aux_df = df.query('category==@i')\n",
    "        aux_df.reset_index(inplace=True)\n",
    "        first_df = first_df.append(aux_df.iloc[0], ignore_index=True)\n",
    "        first_df = first_df.append(aux_df.iloc[1], ignore_index=True)\n",
    "        second_df = second_df.append(aux_df.iloc[2], ignore_index=True)\n",
    "\n",
    "    first_df.reset_index(inplace=True)\n",
    "    first_df = pd.DataFrame(first_df['text_lemmatized'])\n",
    "\n",
    "    second_df.reset_index(inplace=True)\n",
    "    second_df = pd.DataFrame(second_df['text_lemmatized'])\n",
    "\n",
    "    return first_df, second_df\n"
   ]
  },
  {
   "source": [
    "The following function returns a list containing all the words in the new created dataset that are to be labeled"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createVocab(first_df):\n",
    "    documents = []\n",
    "    for i in range(len(first_df.index)):\n",
    "        for x in first_df.iloc[i]['text_lemmatized']:\n",
    "            documents.append(x)\n",
    "    return documents\n",
    "# print(documents)"
   ]
  },
  {
   "source": [
    "The labelDocs function labels the data using all the provided words and transforms each word in each document with the corresponding number.\n",
    "\n",
    "For the second dataframe, the words that were to be processed consisted only in those already labeled by the encoder. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelDocs(first_df, second_df, vocab):\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    first_df_copy = first_df.copy()\n",
    "    second_df_copy = second_df.copy()\n",
    "    le.fit(vocab)\n",
    "\n",
    "    for i in range(len(first_df_copy.index)):\n",
    "        first_df_copy.iloc[i]['text_lemmatized'] = le.transform(first_df_copy.iloc[i]['text_lemmatized'])\n",
    "\n",
    "    for i in range(len(second_df_copy.index)):\n",
    "        list_words = second_df_copy.iloc[i]['text_lemmatized']\n",
    "        list_words = [word for word in list_words if word in vocab]\n",
    "        second_df_copy.iloc[i]['text_lemmatized'] = le.transform(list_words)\n",
    "\n",
    "    first_df_copy = first_df_copy.to_numpy().reshape(-1)\n",
    "    second_df_copy = second_df_copy.to_numpy().reshape(-1)\n",
    "\n",
    "    return first_df_copy, second_df_copy"
   ]
  },
  {
   "source": [
    "The below function is used to calculate the M, N, V, K variables required for the LDA. As described above:\n",
    "\n",
    "- K is the number of topics\n",
    "- V is the size of the vocabulary\n",
    "- M is the number of documents\n",
    "- N contains the number of words for each document"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDims(df, first_df, first_df_labeled, categories):\n",
    "\n",
    "    M = len(first_df.index)\n",
    "    N= []\n",
    "    for i in range(M):\n",
    "        N.append(len(first_df.iloc[i]['text_lemmatized']))\n",
    "    max_v = 0\n",
    "    for array in first_df_labeled:\n",
    "        if max_v < max(array):\n",
    "            max_v = max(array)\n",
    "    V = max_v + 1\n",
    "    K = len(categories)\n",
    "\n",
    "    print(\"M = {}\".format(M))\n",
    "    print(\"N = {}\".format(N))\n",
    "    print(\"V = {}\".format(V))\n",
    "    print(\"K = {}\".format(K))\n",
    "\n",
    "    return M, N, V, K\n"
   ]
  },
  {
   "source": [
    "The function below returns the top 5 most representative words in each topic according to the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topWords(lda, categories, vocab):\n",
    "    words_array = lda.topic_words()\n",
    "    print(\"The topics are: {} \\n\".format(categories))\n",
    "    for index_topic, index_w_array in enumerate(words_array):\n",
    "        words_topic = []\n",
    "        print(\"Topic {} \\n\".format(index_topic)) \n",
    "        for index_w in index_w_array:\n",
    "            words_topic.append(vocab[index_w])\n",
    "        print(\"Top 5 words: {} \\n\".format(words_topic))"
   ]
  },
  {
   "source": [
    "The cell below is used to create the sanity checks in order to see if the model is performing correctly on data "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = [[0, 1, 0], [1, 0, 1], [0, 1, 1, 0], [2, 3], [2, 3, 3], [2, 3, 3, 2]]\n",
    "data_2 = [[0, 1 ,2 ,3 ,4 ,5, 6 ,7],[0, 8, 9, 10, 11, 3, 12, 13],[14, 15, 16, 2, 17, 18, 19],[20, 12, 21, 22, 23, 24],[25, 26, 27, 3, 9, 28, 18]]\n",
    "\n",
    "def sanity_check_1(data_1):\n",
    "    N = [len(doc) for doc in data_1]\n",
    "    K = 2\n",
    "    V = 4\n",
    "    M = 6\n",
    "    lda_sanity = LDA(data_1, K=K, V=V, N=N, M=M, iterations=40000, burn=1000)\n",
    "    lda_sanity.compile()\n",
    "    lda_sanity.trace()\n",
    "\n",
    "def sanity_check_2(data_2):\n",
    "    N = [len(doc) for doc in data_2]\n",
    "    K = 2\n",
    "    V = 29\n",
    "    M = 5\n",
    "    lda_sanity_2 = LDA(data_2, K=K, V=V, N=N, M=M, iterations=40000, burn=1000)\n",
    "    lda_sanity_2.compile()\n",
    "    lda_sanity_2.trace()"
   ]
  },
  {
   "source": [
    "The below cell, consists of the main function that runs all the above described functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      " ~~~~~~~~~~Task 1~~~~~~~~~~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ~~~~Data~~~~\n",
      "\n",
      "\n",
      "                                     text_lemmatized\n",
      "0  [quarterly, profit, us, media, giant, timewarn...\n",
      "1  [dollar, hit, highest, level, euro, almost, th...\n",
      "2  [christmas, tree, receive, text, message, unve...\n",
      "3  [french, musician, jeanmichel, jarre, perform,...\n",
      "4  [maternity, pay, new, mother, rise, part, new,...\n",
      "5  [information, commissioner, say, urgently, ask...\n",
      "6  [british, hurdler, sarah, claxton, confident, ...\n",
      "7  [sonia, osullivan, indicate, would, like, part...\n",
      "8  [kyrgyz, republic, small, mountainous, state, ...\n",
      "9  [chinese, authorities, close, net, cafes, clos...\n",
      "\n",
      "\n",
      " ~~~~Data Labeled Sample~~~~ \n",
      "\n",
      "\n",
      "[ 772  755 1005  604  412  967  523  960  620  251 1057  374  669  105\n",
      "  507  418   98  837  458  505  207  454   13  837  967  846  389  771\n",
      "  837  822  755  135  670  402  665  755  282 1024  129  553 1007   47\n",
      "  965 1024  846  397  686  853  418  505  137   47  616  386  580  933\n",
      "  389  771  755  582  735  960  771  471  195  846   48  996  755  339\n",
      "  515  822   86  930  505   14  816  466  489  933  661  672  867  393\n",
      "  967  505  242  986  879   48  343  242  458  127  967   32  810  811\n",
      "  379  751 1005  859  340  190  855  178  203  965 1025  389  771  755\n",
      "  888  100   40  345  367  288  845  755  889  449  119  378   28  149\n",
      "  872  216 1057  958  368  367  579  820  985  116  811  400  967  731\n",
      "  755  712  816  432  683  370  712  929  605  338  400  655  426  319\n",
      "  376  155  163  341  818  694  846  967  758  674  302  433   50   32\n",
      "  344  454  815 1039  755  591  967  810    5  695  307  808  500   47\n",
      " 1005  594  796   31  661  705  869  161  249  817  855  195  846  995\n",
      "  328   38  636  868   58  548  807  744  868  501   11 1028    5  249\n",
      "  410  631  765   99  766  915   47  331  801   14  815  115  836  915\n",
      "   47  331  581 1010  915]\n",
      "\n",
      "\n",
      " ~~~~Variables~~~~ \n",
      "\n",
      "\n",
      "M = 10\n",
      "N = [229, 227, 100, 161, 258, 218, 119, 84, 375, 209]\n",
      "V = 1063\n",
      "K = 5\n",
      "\n",
      "\n",
      " ~~~~LDA~~~~ \n",
      "\n",
      "\n",
      " [-----------------100%-----------------] 40001 of 40000 complete in 7226.6 sec\n",
      "\n",
      " Phi\n",
      "\n",
      " [[4.26717587e-04 1.24450806e-03 5.21949136e-05 ... 5.23021617e-06\n",
      "  2.29362349e-04 2.68249619e-03]]\n",
      "\n",
      " [[6.46935266e-05 1.62208343e-04 3.50468242e-03 ... 1.00852902e-04\n",
      "  2.78729866e-04 2.64535861e-03]]\n",
      "\n",
      " [[0.00203959 0.00037238 0.00055755 ... 0.00051506 0.00163598 0.00459863]]\n",
      "\n",
      " [[1.07514804e-03 1.22845164e-03 2.02915539e-05 ... 6.29920906e-05\n",
      "  1.26104237e-03 2.16557019e-03]]\n",
      "\n",
      " [[0.00058549 0.00051338 0.00157902 ... 0.00017304 0.00042333 0.00233391]]\n",
      "\n",
      " Theta\n",
      "\n",
      " [[0.21083188 0.2846112  0.105731   0.12503923 0.27378669]]\n",
      "\n",
      " [[0.18213923 0.13312426 0.25646907 0.27683452 0.15143291]]\n",
      "\n",
      " [[0.13748758 0.307439   0.27101988 0.04303052 0.24102302]]\n",
      "\n",
      " [[0.17551508 0.27657968 0.18047731 0.1854788  0.18194913]]\n",
      "\n",
      " [[0.03463349 0.24863905 0.24895237 0.19544505 0.27233004]]\n",
      "\n",
      " [[0.25119205 0.23841702 0.13567845 0.19157258 0.1831399 ]]\n",
      "\n",
      " [[0.16290822 0.23816955 0.10671227 0.38105624 0.11115372]]\n",
      "\n",
      " [[0.32334735 0.0281728  0.22283742 0.31458202 0.11106041]]\n",
      "\n",
      " [[0.21281676 0.17919682 0.13867423 0.31051986 0.15879233]]\n",
      "\n",
      " [[0.2100149  0.17375083 0.30279285 0.11514077 0.19830065]]\n",
      "\n",
      " Z\n",
      "\n",
      " [4. 2. 1. 3. 1. 0. 1. 2. 3. 2. 3. 2. 1. 3. 1. 1. 4. 3. 2. 2. 2. 1. 1. 3.\n",
      " 2. 3. 2. 2. 1. 1. 2. 0. 1. 1. 2. 2. 4. 3. 4. 0. 3. 1. 1. 3. 2. 3. 1. 3.\n",
      " 1. 1. 2. 4. 1. 2. 1. 2. 4. 1. 2. 2. 2. 2. 4. 1. 1. 1. 3. 4. 2. 0. 2. 1.\n",
      " 4. 2. 1. 4. 1. 1. 0. 3. 3. 4. 3. 1. 4. 1. 4. 2. 1. 1. 3. 4. 2. 1. 4. 1.\n",
      " 1. 1. 4. 2. 0. 1. 2. 1. 3. 3. 3. 2. 2. 4. 1. 2. 2. 3. 2. 1. 1. 4. 2. 2.\n",
      " 3. 3. 1. 1. 0. 2. 1. 1. 2. 2. 4. 2. 4. 0. 4. 1. 1. 4. 3. 2. 2. 3. 2. 2.\n",
      " 1. 1. 2. 2. 3. 0. 1. 3. 3. 1. 2. 2. 4. 3. 2. 2. 3. 2. 4. 2. 1. 3. 2. 0.\n",
      " 1. 1. 2. 2. 2. 0. 3. 2. 2. 1. 2. 1. 1. 4. 0. 3. 0. 1. 2. 3. 4. 1. 2. 1.\n",
      " 2. 2. 1. 1. 0. 1. 1. 1. 1. 2. 2. 3. 1. 1. 2. 4. 3. 1. 3. 1. 3. 1. 2. 1.\n",
      " 1. 2. 4. 1. 2. 1. 1. 1. 4. 2. 4. 4. 1.]\n",
      "\n",
      " [2. 2. 3. 2. 3. 1. 2. 3. 3. 2. 2. 2. 4. 3. 2. 4. 3. 2. 3. 3. 0. 2. 3. 2.\n",
      " 2. 2. 2. 3. 3. 1. 2. 2. 3. 3. 2. 1. 2. 2. 2. 3. 2. 1. 1. 2. 3. 3. 3. 1.\n",
      " 0. 3. 2. 1. 1. 3. 2. 3. 2. 3. 1. 1. 3. 2. 2. 1. 3. 3. 0. 1. 3. 1. 3. 2.\n",
      " 1. 0. 2. 3. 1. 3. 2. 2. 3. 1. 0. 2. 2. 4. 3. 3. 1. 3. 2. 1. 2. 3. 3. 3.\n",
      " 1. 3. 3. 2. 2. 3. 2. 3. 3. 1. 2. 2. 1. 4. 2. 2. 2. 3. 3. 3. 2. 2. 3. 3.\n",
      " 1. 2. 2. 3. 3. 1. 2. 2. 1. 2. 2. 3. 0. 3. 3. 3. 1. 0. 2. 2. 0. 2. 2. 3.\n",
      " 3. 2. 3. 3. 3. 2. 2. 1. 3. 1. 2. 2. 2. 3. 2. 2. 3. 3. 3. 1. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 1. 2. 2. 3. 2. 3. 2. 2. 2. 2. 4. 1. 2. 2. 1. 2. 2. 2. 2. 3.\n",
      " 1. 0. 2. 3. 2. 1. 1. 2. 2. 0. 2. 3. 1. 3. 2. 1. 1. 2. 2. 3. 3. 2. 3. 2.\n",
      " 2. 2. 2. 3. 2. 2. 1. 3. 2. 0. 2.]\n",
      "\n",
      " [3. 2. 3. 1. 2. 1. 2. 2. 1. 2. 1. 1. 2. 1. 2. 1. 2. 2. 1. 2. 2. 1. 1. 2.\n",
      " 1. 3. 1. 1. 1. 1. 2. 2. 1. 1. 2. 2. 4. 2. 2. 3. 1. 1. 3. 1. 3. 1. 2. 1.\n",
      " 2. 1. 1. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2. 3. 1. 2. 1. 3. 4. 3. 2. 2. 2. 2.\n",
      " 4. 3. 1. 2. 4. 2. 3. 1. 2. 4. 3. 3. 3. 2. 2. 2. 1. 1. 2. 3. 3. 4. 2. 1.\n",
      " 3. 3. 2. 2.]\n",
      "\n",
      " [2. 2. 1. 2. 3. 3. 2. 3. 1. 0. 1. 1. 1. 1. 2. 3. 2. 3. 3. 1. 2. 2. 2. 2.\n",
      " 1. 2. 1. 0. 2. 3. 2. 1. 3. 2. 2. 3. 3. 2. 4. 1. 1. 2. 3. 1. 0. 3. 1. 3.\n",
      " 1. 1. 3. 2. 2. 1. 4. 1. 3. 3. 2. 1. 1. 1. 3. 2. 2. 1. 3. 1. 3. 2. 0. 1.\n",
      " 2. 0. 1. 4. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 4. 0. 1. 1. 4. 1. 1. 2. 4. 1.\n",
      " 1. 1. 1. 4. 2. 2. 3. 1. 1. 2. 1. 1. 2. 3. 1. 2. 2. 3. 2. 1. 3. 1. 2. 2.\n",
      " 1. 2. 2. 2. 1. 2. 2. 2. 1. 3. 2. 1. 2. 4. 2. 2. 1. 1. 1. 1. 3. 3. 1. 2.\n",
      " 1. 2. 3. 2. 2. 2. 3. 2. 2. 1. 2. 2. 1. 2. 3. 2. 1.]\n",
      "\n",
      " [2. 4. 2. 3. 3. 2. 3. 1. 2. 3. 2. 2. 3. 2. 1. 3. 3. 3. 4. 2. 3. 1. 2. 2.\n",
      " 2. 2. 2. 2. 3. 3. 3. 2. 4. 2. 2. 3. 2. 3. 1. 2. 3. 3. 2. 2. 3. 2. 4. 4.\n",
      " 1. 3. 2. 2. 4. 2. 3. 1. 2. 2. 4. 3. 2. 3. 4. 2. 2. 2. 3. 4. 2. 2. 3. 3.\n",
      " 3. 2. 1. 3. 1. 3. 4. 3. 3. 3. 2. 3. 3. 3. 2. 2. 4. 3. 1. 3. 1. 2. 4. 2.\n",
      " 3. 3. 2. 3. 3. 2. 2. 1. 2. 2. 4. 2. 4. 1. 1. 3. 3. 2. 1. 3. 3. 3. 3. 3.\n",
      " 3. 3. 2. 1. 3. 3. 3. 2. 1. 2. 3. 2. 2. 3. 2. 4. 2. 3. 2. 2. 2. 2. 1. 2.\n",
      " 2. 2. 2. 2. 3. 2. 2. 2. 3. 2. 1. 1. 2. 1. 4. 3. 4. 3. 2. 2. 2. 2. 2. 2.\n",
      " 3. 3. 4. 2. 2. 3. 3. 4. 4. 2. 3. 3. 3. 3. 1. 2. 2. 3. 3. 3. 1. 1. 2. 3.\n",
      " 1. 3. 2. 2. 2. 3. 3. 1. 2. 2. 2. 3. 3. 3. 3. 3. 2. 2. 2. 3. 2. 4. 2. 3.\n",
      " 2. 2. 3. 3. 2. 2. 2. 2. 1. 3. 3. 2. 2. 1. 1. 4. 2. 2. 4. 3. 1. 3. 3. 4.\n",
      " 3. 2. 3. 1. 2. 3. 3. 2. 3. 2. 2. 4. 4. 3. 3. 3. 1. 2.]\n",
      "\n",
      " [2. 3. 1. 2. 3. 1. 1. 3. 3. 2. 2. 2. 2. 2. 3. 2. 1. 0. 1. 0. 2. 2. 1. 3.\n",
      " 3. 2. 0. 3. 1. 3. 1. 2. 1. 3. 2. 2. 3. 1. 2. 4. 1. 1. 3. 0. 1. 1. 0. 2.\n",
      " 3. 1. 3. 4. 1. 4. 2. 2. 1. 3. 1. 2. 3. 1. 2. 1. 2. 2. 2. 2. 1. 2. 0. 2.\n",
      " 3. 1. 1. 2. 2. 1. 1. 2. 0. 1. 2. 1. 3. 2. 2. 2. 1. 2. 2. 1. 1. 1. 1. 4.\n",
      " 4. 1. 1. 3. 2. 3. 4. 1. 0. 0. 1. 2. 1. 4. 4. 3. 1. 1. 2. 3. 2. 2. 3. 1.\n",
      " 1. 2. 4. 0. 2. 1. 1. 2. 2. 0. 3. 3. 1. 2. 2. 2. 3. 2. 3. 2. 1. 3. 1. 1.\n",
      " 2. 0. 2. 2. 1. 3. 3. 3. 0. 1. 0. 1. 1. 0. 2. 1. 0. 0. 4. 4. 0. 2. 3. 2.\n",
      " 2. 1. 1. 2. 3. 1. 4. 1. 1. 1. 3. 3. 2. 2. 1. 1. 2. 3. 2. 1. 3. 2. 2. 1.\n",
      " 3. 3. 2. 1. 0. 3. 2. 2. 3. 3. 2. 3. 1. 1. 1. 1. 3. 1. 3. 2. 3. 3. 2. 1.\n",
      " 2. 1.]\n",
      "\n",
      " [3. 2. 3. 2. 2. 3. 3. 3. 3. 0. 3. 2. 1. 1. 1. 3. 1. 3. 3. 1. 2. 2. 1. 2.\n",
      " 3. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 1. 2. 1. 3. 3. 3. 2. 1. 3. 2. 2.\n",
      " 2. 3. 2. 3. 1. 2. 3. 2. 2. 2. 3. 1. 1. 3. 1. 2. 2. 2. 3. 2. 2. 2. 2. 3.\n",
      " 3. 2. 3. 2. 3. 2. 1. 2. 1. 1. 3. 2. 1. 3. 2. 1. 2. 1. 3. 3. 2. 3. 2. 2.\n",
      " 1. 1. 3. 1. 1. 2. 1. 3. 2. 3. 2. 2. 1. 2. 3. 2. 3. 1. 2. 2. 2. 1. 3.]\n",
      "\n",
      " [1. 2. 1. 0. 2. 3. 0. 3. 2. 3. 2. 1. 0. 3. 1. 1. 2. 1. 2. 0. 3. 3. 2. 3.\n",
      " 3. 3. 1. 3. 1. 0. 3. 1. 0. 0. 1. 3. 1. 3. 1. 3. 2. 3. 1. 2. 1. 3. 2. 3.\n",
      " 1. 2. 0. 3. 1. 1. 1. 3. 3. 2. 2. 3. 3. 1. 3. 2. 3. 2. 1. 2. 3. 2. 3. 1.\n",
      " 1. 2. 2. 2. 3. 1. 2. 2. 0. 2. 3. 3.]\n",
      "\n",
      " [1. 3. 2. 3. 3. 2. 3. 1. 3. 3. 3. 3. 2. 0. 3. 1. 3. 3. 1. 3. 2. 1. 3. 1.\n",
      " 3. 3. 0. 3. 1. 2. 3. 1. 3. 2. 2. 3. 2. 3. 1. 2. 3. 2. 4. 2. 1. 1. 1. 0.\n",
      " 3. 0. 1. 3. 3. 1. 1. 4. 1. 3. 2. 1. 0. 3. 0. 1. 3. 2. 2. 1. 2. 1. 2. 3.\n",
      " 3. 2. 2. 3. 0. 1. 2. 2. 0. 1. 1. 2. 3. 3. 2. 2. 2. 3. 2. 2. 3. 3. 4. 2.\n",
      " 3. 2. 0. 2. 2. 1. 3. 2. 0. 3. 3. 3. 2. 3. 3. 1. 3. 4. 4. 2. 3. 1. 1. 3.\n",
      " 2. 3. 1. 3. 3. 3. 1. 1. 2. 4. 3. 1. 3. 2. 2. 3. 2. 2. 0. 1. 3. 2. 3. 3.\n",
      " 4. 3. 1. 1. 1. 0. 2. 1. 3. 2. 2. 2. 2. 2. 3. 3. 1. 0. 3. 2. 1. 3. 1. 1.\n",
      " 2. 0. 3. 3. 1. 2. 3. 3. 3. 2. 2. 1. 3. 0. 1. 2. 2. 3. 2. 3. 2. 4. 0. 2.\n",
      " 1. 3. 3. 2. 2. 1. 1. 2. 3. 2. 0. 3. 2. 1. 3. 1. 0. 0. 2. 3. 2. 3. 1. 3.\n",
      " 3. 1. 2. 0. 3. 0. 2. 3. 2. 2. 3. 0. 1. 2. 0. 1. 1. 1. 0. 2. 2. 2. 0. 3.\n",
      " 0. 3. 2. 1. 2. 0. 4. 0. 2. 4. 3. 3. 1. 0. 3. 2. 1. 3. 2. 2. 3. 4. 3. 3.\n",
      " 3. 2. 3. 1. 1. 2. 2. 2. 3. 2. 2. 3. 0. 3. 2. 2. 3. 2. 1. 3. 2. 1. 1. 1.\n",
      " 2. 2. 1. 2. 1. 2. 4. 3. 0. 2. 2. 3. 1. 1. 2. 3. 3. 3. 1. 3. 2. 1. 1. 1.\n",
      " 3. 1. 3. 1. 3. 0. 2. 4. 3. 2. 4. 2. 2. 3. 3. 2. 1. 2. 3. 2. 2. 1. 2. 2.\n",
      " 3. 2. 3. 4. 2. 2. 3. 3. 0. 2. 3. 0. 3. 1. 1. 0. 1. 1. 2. 2. 2. 3. 3. 3.\n",
      " 3. 3. 2. 2. 2. 1. 2. 3. 2. 3. 0. 3. 2. 4. 1.]\n",
      "\n",
      " [2. 3. 2. 2. 1. 2. 1. 2. 1. 2. 2. 3. 3. 3. 1. 2. 2. 2. 2. 1. 1. 3. 2. 2.\n",
      " 2. 1. 2. 2. 3. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2. 2. 4. 0. 3. 1. 1.\n",
      " 2. 3. 2. 2. 3. 2. 3. 2. 2. 2. 4. 1. 0. 2. 3. 0. 3. 2. 2. 1. 1. 2. 2. 2.\n",
      " 3. 0. 2. 1. 1. 2. 2. 4. 1. 2. 1. 1. 1. 1. 2. 4. 2. 2. 2. 2. 4. 4. 1. 2.\n",
      " 2. 1. 4. 1. 2. 1. 4. 3. 1. 2. 3. 2. 0. 4. 4. 3. 3. 3. 0. 1. 4. 3. 2. 2.\n",
      " 2. 1. 0. 1. 2. 1. 1. 2. 4. 2. 3. 2. 1. 2. 1. 2. 3. 2. 4. 3. 2. 2. 3. 0.\n",
      " 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 3. 1. 3. 1. 4. 2. 2. 2. 2. 2. 2. 3. 4. 4.\n",
      " 0. 1. 1. 4. 2. 2. 2. 2. 3. 2. 1. 4. 2. 2. 1. 3. 0. 2. 1. 1. 2. 0. 1. 2.\n",
      " 2. 1. 2. 1. 1. 1. 2. 2. 2. 2. 3. 2. 2. 2. 2. 1. 1.]\n",
      "\n",
      "\n",
      " ~~~~TOP WORDS~~~~ \n",
      "\n",
      "\n",
      "The topics are: ['business' 'entertainment' 'politics' 'sport' 'tech'] \n",
      "\n",
      "Topic 0 \n",
      "\n",
      "Top 5 words: ['boost', 'manhattan', 'us', 'general', 'jeanmichel'] \n",
      "\n",
      "Topic 1 \n",
      "\n",
      "Top 5 words: ['democrat', 'assistant', 'aol', 'greenspans', 'rule'] \n",
      "\n",
      "Topic 2 \n",
      "\n",
      "Top 5 words: ['thursday', 'users', 'customers', 'friday', 'royal'] \n",
      "\n",
      "Topic 3 \n",
      "\n",
      "Top 5 words: ['business', 'yawn', 'take', 'recycle', 'hewitt'] \n",
      "\n",
      "Topic 4 \n",
      "\n",
      "Top 5 words: ['mermaid', 'give', 'recent', 'learn', 'bloom'] \n",
      "\n",
      "\n",
      "\n",
      " ~~~~~~~~~~Task 2~~~~~~~~~~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ~~~~Similarity between documents of the same topic~~~~\n",
      "Topic 0: \n",
      "[0, 1, 0.8899047719347555] \n",
      "\n",
      "Topic 1: \n",
      "[2, 3, 0.9337145223550644] \n",
      "\n",
      "Topic 2: \n",
      "[4, 5, 0.8747158887370972] \n",
      "\n",
      "Topic 3: \n",
      "[6, 7, 0.845933767555762] \n",
      "\n",
      "Topic 4: \n",
      "[8, 9, 0.9186276916468933] \n",
      "\n",
      "\n",
      "\n",
      " ~~~~Topics assigned to new documents~~~~\n",
      "\n",
      "\n",
      " ~~~~New Doc Sample~~~~ \n",
      "\n",
      "\n",
      "                                     text_lemmatized\n",
      "0  [owners, embattle, russian, oil, giant, yukos,...\n",
      "1  [classic, film, wonderful, life, turn, musical...\n",
      "2  [plan, extend, pay, maternity, leave, beyond, ...\n",
      "3  [maurice, greene, aim, wipe, pain, lose, olymp...\n",
      "4  [microsoft, investigate, trojan, program, atte...\n",
      " \n",
      "\n",
      "\n",
      " ~~~~Assigned Topics~~~~\n",
      "\n",
      "\n",
      "[0, 1, 2, 1, 4]\n",
      "\n",
      "\n",
      " ~~~~True Topics~~~~\n",
      "\n",
      "\n",
      "[0, 1, 2, 3, 4]\n",
      "\n",
      "\n",
      " ~~~~~~~~~~Sanity Checks~~~~~~~~~~ \n",
      "\n",
      " \n",
      "~~~~Sanity Checks 1~~~~ \n",
      "\n",
      " \n",
      " [-----------------100%-----------------] 40000 of 40000 complete in 123.6 sec\n",
      "\n",
      " Phi\n",
      "\n",
      " [[0.29094539 0.29382954 0.18089664 0.23432843]]\n",
      "\n",
      " [[0.22360846 0.21966924 0.25106344 0.30565887]]\n",
      "\n",
      " Theta\n",
      "\n",
      " [[0.56553211 0.43446789]]\n",
      "\n",
      " [[0.55572934 0.44427066]]\n",
      "\n",
      " [[0.56194321 0.43805679]]\n",
      "\n",
      " [[0.45389174 0.54610826]]\n",
      "\n",
      " [[0.43596348 0.56403652]]\n",
      "\n",
      " [[0.43666495 0.56333505]]\n",
      "\n",
      " Z\n",
      "\n",
      " [0. 0. 0.]\n",
      "\n",
      " [0. 0. 0.]\n",
      "\n",
      " [0. 0. 0. 0.]\n",
      "\n",
      " [1. 1.]\n",
      "\n",
      " [1. 1. 1.]\n",
      "\n",
      " [1. 1. 1. 1.]\n",
      "~~~~Sanity Checks 2~~~~ \n",
      "\n",
      " \n",
      " [-----------------100%-----------------] 40000 of 40000 complete in 148.0 sec\n",
      "\n",
      " Phi\n",
      "\n",
      " [[0.01388733 0.03165603 0.03484574 0.04944278 0.02341261 0.00813512\n",
      "  0.02708947 0.02767751 0.03401186 0.05353971 0.03021286 0.03588653\n",
      "  0.07810986 0.03141957 0.00204538 0.04145229 0.03851914 0.04981569\n",
      "  0.04464864 0.04632509 0.0319962  0.03462298 0.04372011 0.05970844\n",
      "  0.01234259 0.04291567 0.03093309 0.00548202 0.03614569]]\n",
      "\n",
      " [[0.0394889  0.04411635 0.08914607 0.08793572 0.00723149 0.03846917\n",
      "  0.03697507 0.02527989 0.01247344 0.03079364 0.0321942  0.03340065\n",
      "  0.03661917 0.0319775  0.05923034 0.02364524 0.00956764 0.03662636\n",
      "  0.03914305 0.0009912  0.0354031  0.03616758 0.0149449  0.0352991\n",
      "  0.03765953 0.02130539 0.03902134 0.03335121 0.03154275]]\n",
      "\n",
      " Theta\n",
      "\n",
      " [[0.3550282 0.6449718]]\n",
      "\n",
      " [[0.51370071 0.48629929]]\n",
      "\n",
      " [[0.54536859 0.45463141]]\n",
      "\n",
      " [[0.53905258 0.46094742]]\n",
      "\n",
      " [[0.44590379 0.55409621]]\n",
      "\n",
      " Z\n",
      "\n",
      " [1. 1. 1. 1. 0. 1. 1. 1.]\n",
      "\n",
      " [1. 0. 0. 1. 0. 1. 0. 0.]\n",
      "\n",
      " [1. 0. 0. 1. 0. 0. 0.]\n",
      "\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      "\n",
      " [0. 1. 1. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'bbc-news-data.csv', sep='\\t')\n",
    "categories = df['category'].unique()\n",
    "df.drop(['filename', 'title'], axis = 1, inplace=True)\n",
    "df['text_lemmatized'] = df.content.apply(lemmatizeText)\n",
    "\n",
    "print(\"\\n\\n ~~~~~~~~~~Task 1~~~~~~~~~~\\n\\n\")\n",
    "\n",
    "print(\"\\n\\n ~~~~Data~~~~\\n\\n\")\n",
    "first_df, second_df = getData(df)\n",
    "print(\"{}\".format(first_df))\n",
    "vocab = createVocab(first_df)\n",
    "\n",
    "print(\"\\n\\n ~~~~Data Labeled Sample~~~~ \\n\\n\")\n",
    "first_df_labeled, second_df_labeled = labelDocs(first_df, second_df, vocab)\n",
    "print(\"{}\".format(first_df_labeled[0]))\n",
    "\n",
    "print(\"\\n\\n ~~~~Variables~~~~ \\n\\n\")\n",
    "M, N, V, K = generateDims(df, first_df, first_df_labeled, categories)\n",
    "\n",
    "print(\"\\n\\n ~~~~LDA~~~~ \\n\\n\")\n",
    "lda = LDA(first_df_labeled, K=K, V=V, N=N, M=M, iterations=40000 ,burn=1000)\n",
    "lda.compile()\n",
    "lda.trace()\n",
    "\n",
    "print(\"\\n\\n ~~~~TOP WORDS~~~~ \\n\\n\")\n",
    "topWords(lda, categories, vocab)\n",
    "\n",
    "print(\"\\n\\n ~~~~~~~~~~Task 2~~~~~~~~~~\\n\\n\")\n",
    "\n",
    "print(\"\\n\\n ~~~~Similarity between documents of the same topic~~~~\")\n",
    "lda.documents_similarity(0.8)\n",
    "\n",
    "print(\"\\n\\n ~~~~Topics assigned to new documents~~~~\")\n",
    "print(\"\\n\\n ~~~~New Doc Sample~~~~ \\n\\n\")\n",
    "print(\"{}\\n \".format(second_df))\n",
    "\n",
    "print(\"\\n\\n ~~~~Assigned Topics~~~~\\n\\n\")\n",
    "lda.assign_topics_new_document(second_df = second_df_labeled)\n",
    "\n",
    "print(\"\\n\\n ~~~~True Topics~~~~\\n\\n\")\n",
    "true_topics = [0, 1, 2, 3, 4]\n",
    "print(true_topics)\n",
    "\n",
    "\n",
    "print(\"\\n\\n ~~~~~~~~~~Sanity Checks~~~~~~~~~~ \\n\\n \")\n",
    "print(\"~~~~Sanity Checks 1~~~~ \\n\\n \")\n",
    "sanity_check_1(data_1)\n",
    "print(\"~~~~Sanity Checks 2~~~~ \\n\\n \")\n",
    "sanity_check_2(data_2)"
   ]
  },
  {
   "source": [
    "As it can bee seen from the logs, if the LDA were to iterate 40000 times, then it would correctly assign topics to 80% of the documents. Moreover, the sanity checks show that the model is working fine. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}